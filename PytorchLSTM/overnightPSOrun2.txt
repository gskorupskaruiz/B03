(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.4579528469, val loss = 0.1621184340
Epoch 2: train loss = 0.2276922446, val loss = 0.1120611606
Epoch 3: train loss = 0.1749382134, val loss = 0.1067986403
Epoch 4: train loss = 0.1678622213, val loss = 0.1157490775
Epoch 5: train loss = 0.1891382729, val loss = 0.1186500968
Epoch 6: train loss = 0.1934377089, val loss = 0.1051161648
Epoch 7: train loss = 0.1582990157, val loss = 0.0898268656
Epoch 8: train loss = 0.1387543145, val loss = 0.0897155152
Epoch 9: train loss = 0.1322817580, val loss = 0.0866601171
Epoch 10: train loss = 0.1306284768, val loss = 0.0832784803
Epoch 11: train loss = 0.1302971543, val loss = 0.0915989335
Epoch 12: train loss = 0.1355259387, val loss = 0.0930231791
Epoch 13: train loss = 0.1354685563, val loss = 0.0889807342
Epoch 14: train loss = 0.1292414464, val loss = 0.0873625002
Epoch 15: train loss = 0.1327184597, val loss = 0.0897005231
Epoch 16: train loss = 0.1302272099, val loss = 0.0895569439
Epoch 17: train loss = 0.1277515418, val loss = 0.0891794249
Epoch 18: train loss = 0.1324205641, val loss = 0.0910100597
Epoch 19: train loss = 0.1304416444, val loss = 0.0865102155
Epoch 20: train loss = 0.1316302438, val loss = 0.0909915074
0.08808952105130588
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.5502899575, val loss = 0.1643430616
Epoch 2: train loss = 0.2515711876, val loss = 0.0985303884
Epoch 3: train loss = 0.1504582786, val loss = 0.0909162376
Epoch 4: train loss = 0.1348329625, val loss = 0.0841722063
Epoch 5: train loss = 0.1343370914, val loss = 0.0800069198
Epoch 6: train loss = 0.1343358628, val loss = 0.0789263969
Epoch 7: train loss = 0.1410319215, val loss = 0.0806979890
Epoch 8: train loss = 0.1328969940, val loss = 0.0792151418
Epoch 9: train loss = 0.1353588058, val loss = 0.0839103636
Epoch 10: train loss = 0.1292340302, val loss = 0.0828107588
Epoch 11: train loss = 0.1385150669, val loss = 0.0777007201
Epoch 12: train loss = 0.1267159239, val loss = 0.0803538293
Epoch 13: train loss = 0.1263888780, val loss = 0.0795027220
Epoch 14: train loss = 0.1295709916, val loss = 0.0773841778
Epoch 15: train loss = 0.1407947132, val loss = 0.0808553447
Epoch 16: train loss = 0.1320801949, val loss = 0.0783775658
Epoch 17: train loss = 0.1339580374, val loss = 0.0808329745
Epoch 18: train loss = 0.1242286309, val loss = 0.0772281217
Epoch 19: train loss = 0.1286563829, val loss = 0.0788135132
Epoch 20: train loss = 0.1254582808, val loss = 0.0814899061
0.0790708257845518
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
YOOOOOOOOOOOOO
[[100, 3, 46, 37], [100, 3, 69, 40]]
[0.08808952105130588, 0.0790708257845518]
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.4110221509, val loss = 0.1460665087
Epoch 2: train loss = 0.1745021686, val loss = 0.1063132964
Epoch 3: train loss = 0.1532753636, val loss = 0.1021978922
Epoch 4: train loss = 0.1436027557, val loss = 0.1121504318
Epoch 5: train loss = 0.1464828176, val loss = 0.1001483442
Epoch 6: train loss = 0.1419706195, val loss = 0.0964455591
Epoch 7: train loss = 0.1421535716, val loss = 0.0973266957
Epoch 8: train loss = 0.1873715381, val loss = 0.1004500760
Epoch 9: train loss = 0.1928439346, val loss = 0.1072679760
Epoch 10: train loss = 0.2532554439, val loss = 0.1210243004
Epoch 11: train loss = 0.1616705082, val loss = 0.1034870416
Epoch 12: train loss = 0.1415614810, val loss = 0.1000542590
Epoch 13: train loss = 0.1455596937, val loss = 0.0981469971
Epoch 14: train loss = 0.1465349036, val loss = 0.1051717472
Epoch 15: train loss = 0.1408060963, val loss = 0.1024832803
Epoch 16: train loss = 0.1667464231, val loss = 0.0988529506
Epoch 17: train loss = 0.1791276749, val loss = 0.1057127128
Epoch 18: train loss = 0.1846319420, val loss = 0.0979656810
Epoch 19: train loss = 0.1696203195, val loss = 0.1039472069
Epoch 20: train loss = 0.1588011722, val loss = 0.0975501935
0.10145459425453002
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 1.0741923956, val loss = 0.8339520735
Epoch 2: train loss = 1.0470641117, val loss = 0.8327686356
Epoch 3: train loss = 1.0491836733, val loss = 0.8331657659
Epoch 4: train loss = 1.0459166072, val loss = 0.8322369995
Epoch 5: train loss = 1.0412037447, val loss = 0.7608027517
Epoch 6: train loss = 0.6479786672, val loss = 0.5767387525
Epoch 7: train loss = 0.4452727019, val loss = 0.2735056357
Epoch 8: train loss = 0.4482718269, val loss = 0.2524995864
Epoch 9: train loss = 0.3159597009, val loss = 0.2179621189
Epoch 10: train loss = 0.3443696871, val loss = 0.2203590853
Epoch 11: train loss = 0.3377734529, val loss = 0.2004161774
Epoch 12: train loss = 0.2897810181, val loss = 0.1899564177
Epoch 13: train loss = 0.3204743757, val loss = 0.2070369513
Epoch 14: train loss = 0.3237208279, val loss = 0.2127533708
Epoch 15: train loss = 0.2953883532, val loss = 0.1787077600
Epoch 16: train loss = 0.2860651231, val loss = 0.1810672714
Epoch 17: train loss = 0.2568617062, val loss = 0.1812592508
Epoch 18: train loss = 0.2605447768, val loss = 0.1792900547
Epoch 19: train loss = 0.2781675395, val loss = 0.1780870136
Epoch 20: train loss = 0.2640558351, val loss = 0.1947409404
0.2052658161737076
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.6790337730, val loss = 0.3004262162
Epoch 2: train loss = 0.3699387617, val loss = 0.1887272274
Epoch 3: train loss = 0.2277102217, val loss = 0.1136776079
Epoch 4: train loss = 0.1741438419, val loss = 0.1027476716
Epoch 5: train loss = 0.1618935856, val loss = 0.0931970556
Epoch 6: train loss = 0.1342409337, val loss = 0.0855359811
Epoch 7: train loss = 0.1315394307, val loss = 0.0806149965
Epoch 8: train loss = 0.1447855839, val loss = 0.0853448080
Epoch 9: train loss = 0.1398313918, val loss = 0.0779640892
Epoch 10: train loss = 0.1349004588, val loss = 0.0818760351
Epoch 11: train loss = 0.1439446800, val loss = 0.0784960765
Epoch 12: train loss = 0.1269019247, val loss = 0.0760554153
Epoch 13: train loss = 0.1242869729, val loss = 0.0793497247
Epoch 14: train loss = 0.1271407813, val loss = 0.0728394670
Epoch 15: train loss = 0.1336203526, val loss = 0.0779813371
Epoch 16: train loss = 0.1354007620, val loss = 0.0818153133
Epoch 17: train loss = 0.1312597763, val loss = 0.0844355267
Epoch 18: train loss = 0.1288065843, val loss = 0.0748651318
Epoch 19: train loss = 0.1318594518, val loss = 0.0817182518
Epoch 20: train loss = 0.1284490732, val loss = 0.0775458742
0.07420530135310223
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
YOOOOOOOOOOOOO
[[100, 3, 46, 37], [100, 3, 69, 40], [100, 3, 75, 40]]
[0.08808952105130588, 0.0790708257845518, 0.07420530135310223]
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.8193014724, val loss = 0.2802988701
Epoch 2: train loss = 0.3991095965, val loss = 0.2588496137
Epoch 3: train loss = 0.3532954084, val loss = 0.2416048607
Epoch 4: train loss = 0.3385861846, val loss = 0.2336562102
Epoch 5: train loss = 0.3323068198, val loss = 0.2425192956
Epoch 6: train loss = 0.3169543867, val loss = 0.2335139968
Epoch 7: train loss = 0.3126646864, val loss = 0.2332362062
Epoch 8: train loss = 0.3212113475, val loss = 0.2223024661
Epoch 9: train loss = 0.3117427464, val loss = 0.2459466456
Epoch 10: train loss = 0.2862379602, val loss = 0.1539835926
Epoch 11: train loss = 0.3222323024, val loss = 0.2495603564
Epoch 12: train loss = 0.3675005789, val loss = 0.1845336812
Epoch 13: train loss = 0.2198635136, val loss = 0.1594977751
Epoch 14: train loss = 0.2213307315, val loss = 0.1505109247
Epoch 15: train loss = 0.2291067082, val loss = 0.1488127011
Epoch 16: train loss = 0.2333238733, val loss = 0.1478519527
Epoch 17: train loss = 0.2220877186, val loss = 0.1615633356
Epoch 18: train loss = 0.2292479837, val loss = 0.1659349366
Epoch 19: train loss = 0.2433914803, val loss = 0.1545195385
Epoch 20: train loss = 0.2317565212, val loss = 0.1616460425
0.15333829407203692
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.9695705156, val loss = 0.3162572491
Epoch 2: train loss = 0.3030264438, val loss = 0.1659448994
Epoch 3: train loss = 0.2273361753, val loss = 0.1417135064
Epoch 4: train loss = 0.2154763011, val loss = 0.1325198714
Epoch 5: train loss = 0.1778267152, val loss = 0.1152895656
Epoch 6: train loss = 0.1849290895, val loss = 0.1142342900
Epoch 7: train loss = 0.1581671318, val loss = 0.1199914290
Epoch 8: train loss = 0.1471458823, val loss = 0.1077626646
Epoch 9: train loss = 0.1378423737, val loss = 0.1015124384
Epoch 10: train loss = 0.1407748801, val loss = 0.1014345566
Epoch 11: train loss = 0.1797067882, val loss = 0.1271213156
Epoch 12: train loss = 0.1572402121, val loss = 0.0988624227
Epoch 13: train loss = 0.1679230356, val loss = 0.0995229537
Epoch 14: train loss = 0.1579832204, val loss = 0.0998709690
Epoch 15: train loss = 0.1446312149, val loss = 0.0941114917
Epoch 16: train loss = 0.1520084700, val loss = 0.0979038407
Epoch 17: train loss = 0.1379257270, val loss = 0.0977786351
Epoch 18: train loss = 0.1225512827, val loss = 0.0891513643
Epoch 19: train loss = 0.1392986560, val loss = 0.0964695111
Epoch 20: train loss = 0.1433491366, val loss = 0.0902174680
0.09632466136189997
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 0.5229831925, val loss = 0.2033468149
Epoch 2: train loss = 0.2387801162, val loss = 0.1327760208
Epoch 3: train loss = 0.2085016449, val loss = 0.1198085158
Epoch 4: train loss = 0.1832078745, val loss = 0.1210505346
Epoch 5: train loss = 0.1754863762, val loss = 0.1263348594
Epoch 6: train loss = 0.2006849103, val loss = 0.1248233761
Epoch 7: train loss = 0.1711084994, val loss = 0.1171356510
Epoch 8: train loss = 0.1824426927, val loss = 0.1354695390
Epoch 9: train loss = 0.1808899149, val loss = 0.1210154199
Epoch 10: train loss = 0.1639906751, val loss = 0.1256030861
Epoch 11: train loss = 0.1706990509, val loss = 0.1210338616
Epoch 12: train loss = 0.1691167506, val loss = 0.1153836048
Epoch 13: train loss = 0.1584274357, val loss = 0.1051351061
Epoch 14: train loss = 0.1561710721, val loss = 0.1065177209
Epoch 15: train loss = 0.1462933887, val loss = 0.1059171555
Epoch 16: train loss = 0.1525913512, val loss = 0.1056863794
Epoch 17: train loss = 0.1543016655, val loss = 0.1059193620
Epoch 18: train loss = 0.1589401993, val loss = 0.1117659028
Epoch 19: train loss = 0.1541981008, val loss = 0.1059105502
Epoch 20: train loss = 0.1504756562, val loss = 0.1044414278
0.11153114882200159
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
[100, 3, 75, 40] 0.07420530135310223
(40000, 7) (5000, 7) (5000, 7)
torch.Size([39980, 1, 1])
GPU is availible:  False
Epoch 1: train loss = 1.1205058362, val loss = 0.4770702448
Epoch 2: train loss = 0.4986311895, val loss = 0.3915567216
Epoch 3: train loss = 0.4793263202, val loss = 0.3393551849
Epoch 4: train loss = 0.4495661454, val loss = 0.3470052569
Epoch 5: train loss = 0.4386887875, val loss = 0.3446179415
Epoch 6: train loss = 0.4456410828, val loss = 0.3481846231
Epoch 7: train loss = 0.4382078446, val loss = 0.3249639341
Epoch 8: train loss = 0.4348671352, val loss = 0.3369600938
Epoch 9: train loss = 0.4397458814, val loss = 0.3364259094
Epoch 10: train loss = 0.4446166133, val loss = 0.3332053440
Epoch 11: train loss = 0.4401312603, val loss = 0.3546441711
Epoch 12: train loss = 0.4509733014, val loss = 0.3611892275
Epoch 13: train loss = 0.4541176391, val loss = 0.3491246519
Epoch 14: train loss = 0.4585203929, val loss = 0.3413460258
Epoch 15: train loss = 0.4919981031, val loss = 0.3593942094
Epoch 16: train loss = 0.4716011933, val loss = 0.3741251190
Epoch 17: train loss = 0.4756454490, val loss = 0.3094809993
Epoch 18: train loss = 0.3467484114, val loss = 0.1839591732
Epoch 19: train loss = 0.2506800133, val loss = 0.1827536472
Epoch 20: train loss = 0.2452976595, val loss = 0.1773920134
0.1845945546405409
ParametricCNNLSTM(
  (lstm): LSTM(7, 40, num_layers=2, batch_first=True, dropout=0.2)
  (dense1): Linear(in_features=40, out_features=20, bias=True)
  (conv1): Conv1d(20, 32, kernel_size=(2,), stride=(1,))
  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv1d(32, 10, kernel_size=(1,), stride=(1,), padding=(2,))
  (batch2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv1d(10, 1, kernel_size=(2,), stride=(1,), padding=(2,))
  (batch3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense2): Linear(in_features=26, out_features=10, bias=True)
  (dense3): Linear(in_features=10, out_features=1, bias=True)
  (denseLast): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2, inplace=False)
)
DONEEEEEEEEEEEEEEE
[100, 3, 75, 40] 0.1845945546405409
